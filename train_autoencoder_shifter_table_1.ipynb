{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Subset, DataLoader, ConcatDataset\n",
    "from mouse_model.data_utils_new import MouseDatasetSegNewBehav\n",
    "import numpy as np\n",
    "from mouse_model.evaluation import cor_in_time\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import random, os\n",
    "from kornia.geometry.transform import get_affine_matrix2d, warp_affine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful for printing in nn.Sequential\n",
    "class PrintLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(PrintLayer, self).__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        return x\n",
    "\n",
    "def size_helper(in_length, kernel_size, padding=0, dilation=1, stride=1):\n",
    "    # https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d\n",
    "    res = in_length + 2 * padding - dilation * (kernel_size - 1) - 1\n",
    "    res /= stride\n",
    "    res += 1\n",
    "    return np.floor(res)\n",
    "\n",
    "# CNN, the last fully connected layer maps to output_dim\n",
    "class VisualEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, output_dim, input_shape=(60, 80), k1=3, k2=3, k3=3, c1=32, c2=64, c3=128, dropout_prob=0):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_shape = (60, 80)\n",
    "        out_shape_0 = size_helper(in_length=input_shape[0], kernel_size=k1, stride=2)\n",
    "        out_shape_0 = size_helper(in_length=out_shape_0, kernel_size=k2, stride=2)\n",
    "        out_shape_0 = size_helper(in_length=out_shape_0, kernel_size=k3, stride=2)\n",
    "        out_shape_1 = size_helper(in_length=input_shape[1], kernel_size=k1, stride=2)\n",
    "        out_shape_1 = size_helper(in_length=out_shape_1, kernel_size=k2, stride=2)\n",
    "        out_shape_1 = size_helper(in_length=out_shape_1, kernel_size=k3, stride=2)\n",
    "        self.output_shape = (int(out_shape_0), int(out_shape_1)) # shape of the final feature map\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=k1, stride=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=k2, stride=2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=k3, stride=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.output_shape[0]*self.output_shape[1]*c3, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.layers(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualDecoder777(nn.Module):\n",
    "    \n",
    "    # input_dim is latent size\n",
    "    def __init__(self, input_dim, c1=128, c2=64, c3=32, dropout_prob=0):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, c1*3*5),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.Unflatten(1, (c1, 3, 5)),\n",
    "#             PrintLayer(),\n",
    "            nn.ConvTranspose2d(in_channels=c1, out_channels=c2, kernel_size=7, stride=2, output_padding=(0,1)),\n",
    "#             PrintLayer(),\n",
    "            nn.BatchNorm2d(c2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.ConvTranspose2d(in_channels=c2, out_channels=c3, kernel_size=7, stride=2),\n",
    "#             PrintLayer(),\n",
    "            nn.BatchNorm2d(c3),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.ConvTranspose2d(in_channels=c3, out_channels=1, kernel_size=7, stride=2, output_padding=1),\n",
    "#             PrintLayer(),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv_layers(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Shifter(nn.Module):\n",
    "    def __init__(self, input_dim=4, output_dim=3, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.BatchNorm1d(input_dim),\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.bias = nn.Parameter(torch.zeros(3))\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1,self.input_dim )\n",
    "        x = self.layers(x)\n",
    "        x0 = (x[...,0] + self.bias[0]) * 80/4\n",
    "        x1 = (x[...,1] + self.bias[1]) * 60/4\n",
    "        x2 = (x[...,2] + self.bias[2]) * 180/4\n",
    "        x = torch.stack([x0, x1, x2], dim=-1)\n",
    "        x = x.reshape(-1,1,self.output_dim)\n",
    "        return x\n",
    "        \n",
    "class AutoencoderPredictor777(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_neurons, latent_size, c1=32, c2=64, c3=128, dropout_prob=0):\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = VisualEncoder(output_dim=latent_size, k1=7, k2=7, k3=7, \n",
    "                                     c1=c1, c2=c2, c3=c3, dropout_prob=dropout_prob)\n",
    "        self.decoder = VisualDecoder777(input_dim=latent_size, c1=c3, c2=c2, c3=c1, dropout_prob=dropout_prob)\n",
    "        self.fc = nn.Linear(latent_size, num_neurons)\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.shifter = Shifter()\n",
    "\n",
    "    def forward(self, images, behav):\n",
    "        if args.shifter:\n",
    "            bs = images.size()[0]\n",
    "            behav_shifter = torch.concat((behav[...,4].unsqueeze(-1),   # theta\n",
    "                                          behav[...,3].unsqueeze(-1),   # phi\n",
    "                                          behav[...,1].unsqueeze(-1),  # pitch\n",
    "                                         behav[...,2].unsqueeze(-1),  # roll\n",
    "                                         ), dim=-1)  \n",
    "            shift_param = self.shifter(behav_shifter)  \n",
    "            shift_param = shift_param.reshape(-1,3)\n",
    "            scale_param = torch.ones_like(shift_param[..., 0:2]).to(shift_param.device)\n",
    "            affine_mat = get_affine_matrix2d(\n",
    "                                            translations=shift_param[..., 0:2] ,\n",
    "                                             scale = scale_param, \n",
    "                                             center =torch.repeat_interleave(torch.tensor([[30,40]], dtype=torch.float), \n",
    "                                                                            bs*1, dim=0).to(shift_param.device), \n",
    "                                             angle=shift_param[..., 2])\n",
    "            affine_mat = affine_mat[:, :2, :]\n",
    "            images = warp_affine(images, affine_mat, dsize=(60,80))\n",
    "\n",
    "        latent_vec = self.encoder(images)\n",
    "        recon = self.decoder(latent_vec)\n",
    "        pred = self.fc(latent_vec)\n",
    "        pred = self.softplus(pred)\n",
    "        \n",
    "        return pred, recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    \n",
    "    seed = 0\n",
    "    batch_size = 256\n",
    "    learning_rate = 0.0001\n",
    "    epochs = 50\n",
    "    alpha = 0.5\n",
    "    file_id = None\n",
    "    num_neurons = None\n",
    "    behav_mode = \"orig\"\n",
    "    best_val_path = None\n",
    "    best_train_path = None\n",
    "    vid_type = \"vid_mean\"\n",
    "    segment_num = 10\n",
    "    seq_len = 1\n",
    "    vis_latent_dim = 256\n",
    "    shifter=True\n",
    "    \n",
    "args=Args()\n",
    "\n",
    "seed = args.seed\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_val_ds():\n",
    "    ds_list = [MouseDatasetSegNewBehav(\n",
    "        file_id=args.file_id, seg_idx=i, segment_num=args.segment_num, data_split=\"train\", vid_type=args.vid_type, \n",
    "        seq_len=args.seq_len, predict_offset=1, behav_mode=args.behav_mode, norm_mode=\"01\") \n",
    "               for i in range(args.segment_num)]\n",
    "    train_ds, val_ds = [], []\n",
    "    for ds in ds_list:\n",
    "        train_ratio = 0.8\n",
    "        train_ds_len = int(len(ds) * train_ratio)\n",
    "        train_ds.append(Subset(ds, np.arange(0, train_ds_len, 1)))\n",
    "        val_ds.append(Subset(ds, np.arange(train_ds_len, len(ds), 1)))\n",
    "    train_ds = ConcatDataset(train_ds)\n",
    "    val_ds = ConcatDataset(val_ds)\n",
    "    print(len(train_ds), len(val_ds))\n",
    "    return train_ds, val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_ds():\n",
    "    test_ds = [MouseDatasetSegNewBehav(\n",
    "        file_id=args.file_id, seg_idx=i, segment_num=args.segment_num, data_split=\"test\", vid_type=args.vid_type, \n",
    "        seq_len=args.seq_len, predict_offset=1, behav_mode=args.behav_mode, norm_mode=\"01\") \n",
    "               for i in range(args.segment_num)]\n",
    "    test_ds = ConcatDataset(test_ds)\n",
    "    return test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    \n",
    "    torch.manual_seed(args.seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(args.seed)\n",
    "    \n",
    "    train_ds, val_ds = load_train_val_ds()\n",
    "\n",
    "    train_dataloader = DataLoader(dataset=train_ds, batch_size=args.batch_size, shuffle=True, num_workers=8)\n",
    "    val_dataloader = DataLoader(dataset=val_ds, batch_size=args.batch_size, shuffle=False, num_workers=8)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "    best_train_spike_loss = np.inf\n",
    "    best_val_spike_loss = np.inf\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "\n",
    "    ct = 0\n",
    "    \n",
    "    # start training\n",
    "    for epoch in range(args.epochs):\n",
    "\n",
    "        print(\"Start epoch\", epoch)\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        epoch_train_loss, epoch_train_spike_loss, epoch_train_recon_loss = 0, 0, 0\n",
    "\n",
    "        for (image, behav, spikes) in train_dataloader:\n",
    "\n",
    "            image, behav, spikes = image.to(device), behav.to(device), spikes.to(device)\n",
    "            image = torch.squeeze(image, axis=1)\n",
    "            \n",
    "            pred, recon = model(image, behav)\n",
    "            \n",
    "            recon_loss = torch.nn.functional.mse_loss(recon, image, reduction='mean')\n",
    "            spike_loss = nn.functional.poisson_nll_loss(pred, spikes, reduction='mean', log_input=False)\n",
    "            total_loss = spike_loss + args.alpha * recon_loss\n",
    "\n",
    "            epoch_train_loss += total_loss.item()\n",
    "            epoch_train_spike_loss += spike_loss.item()\n",
    "            epoch_train_recon_loss += recon_loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        epoch_train_loss = epoch_train_loss / len(train_dataloader)\n",
    "        epoch_train_spike_loss = epoch_train_spike_loss / len(train_dataloader)\n",
    "        epoch_train_recon_loss = epoch_train_recon_loss / len(train_dataloader)\n",
    "\n",
    "        train_loss_list.append([epoch_train_loss, epoch_train_spike_loss, epoch_train_recon_loss])\n",
    "        \n",
    "        print(\"Epoch {} train loss: {}, spike loss: {}, recon loss: {}\".format(\n",
    "            epoch, epoch_train_loss, epoch_train_spike_loss, epoch_train_recon_loss))\n",
    "\n",
    "        if epoch_train_spike_loss < best_train_spike_loss:\n",
    "\n",
    "            print(\"save train model at epoch\", epoch)\n",
    "            torch.save(model.state_dict(), args.best_train_path)\n",
    "            best_train_spike_loss = epoch_train_spike_loss\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        epoch_val_loss, epoch_val_spike_loss, epoch_val_recon_loss = 0, 0, 0\n",
    "\n",
    "        with torch.no_grad():      \n",
    "\n",
    "            for (image, behav, spikes) in val_dataloader:\n",
    "\n",
    "                image, behav, spikes = image.to(device), behav.to(device), spikes.to(device)\n",
    "                image = torch.squeeze(image, axis=1)\n",
    "\n",
    "                pred, recon = model(image, behav)\n",
    "                \n",
    "                recon_loss = torch.nn.functional.mse_loss(recon, image, reduction='mean')\n",
    "                spike_loss = nn.functional.poisson_nll_loss(pred, spikes, reduction='mean', log_input=False)\n",
    "                total_loss = spike_loss + args.alpha * recon_loss\n",
    "\n",
    "                epoch_val_loss += total_loss.item()\n",
    "                epoch_val_spike_loss += spike_loss.item()\n",
    "                epoch_val_recon_loss += recon_loss.item()\n",
    "                \n",
    "        epoch_val_loss = epoch_val_loss / len(val_dataloader)\n",
    "        epoch_val_spike_loss = epoch_val_spike_loss / len(val_dataloader)\n",
    "        epoch_val_recon_loss = epoch_val_recon_loss / len(val_dataloader)\n",
    "\n",
    "        val_loss_list.append([epoch_val_loss, epoch_val_spike_loss, epoch_val_recon_loss])\n",
    "        \n",
    "        print(\"Epoch {} val spike loss: {}\".format(epoch, epoch_val_spike_loss))\n",
    "\n",
    "        if epoch_val_spike_loss < best_val_spike_loss:\n",
    "\n",
    "            print(\"save val model at epoch\", epoch)\n",
    "            torch.save(model.state_dict(), args.best_val_path)\n",
    "            best_val_spike_loss = epoch_val_spike_loss\n",
    "            ct = 0\n",
    "            \n",
    "        else:\n",
    "            ct += 1\n",
    "            if ct > 5:\n",
    "                print('stop training')\n",
    "                break\n",
    "\n",
    "        print(\"End epoch\", epoch)\n",
    "        \n",
    "    return train_loss_list, val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "070921_J553RT\n"
     ]
    },
    {
     "ename": "AcceleratorError",
     "evalue": "CUDA error: invalid device ordinal\nGPU device may be out of range, do you have enough GPUs?\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAcceleratorError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     11\u001b[39m     args.best_train_path = \u001b[33m\"\u001b[39m\u001b[33m/hdd/yuchen/trainAEshifter\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m.pth\u001b[39m\u001b[33m\"\u001b[39m.format(args.shifter, \u001b[32m1\u001b[39m, args.vid_type, file_id)\n\u001b[32m     12\u001b[39m     args.best_val_path = \u001b[33m\"\u001b[39m\u001b[33m/hdd/yuchen/valAEshifter\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m.pth\u001b[39m\u001b[33m\"\u001b[39m.format(args.shifter, \u001b[32m1\u001b[39m, args.vid_type, file_id)\n\u001b[32m     14\u001b[39m     model = \u001b[43mAutoencoderPredictor777\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_neurons\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_neurons\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m                                    \u001b[49m\u001b[43mlatent_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvis_latent_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43m                                    \u001b[49m\u001b[43mc1\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc3\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m     train_loss_list, val_loss_list = train_model()\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# np.save(\"/hdd/aiwenxu/net_weights_final/vis_autoencoder/loss_777_c1_64_{}.npy\".format(file_id), \u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m#         np.array([train_loss_list, val_loss_list]))\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/bioV/2023-Xu-Multimodal-Mouse-V1/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1381\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1378\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1379\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1381\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/bioV/2023-Xu-Multimodal-Mouse-V1/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:933\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    931\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    932\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m933\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_subclasses\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfake_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FakeTensor\n\u001b[32m    937\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied) -> \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/bioV/2023-Xu-Multimodal-Mouse-V1/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:933\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    931\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    932\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m933\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_subclasses\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfake_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FakeTensor\n\u001b[32m    937\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied) -> \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/bioV/2023-Xu-Multimodal-Mouse-V1/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:933\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    931\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    932\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m933\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_subclasses\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfake_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FakeTensor\n\u001b[32m    937\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied) -> \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/bioV/2023-Xu-Multimodal-Mouse-V1/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:964\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    960\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    961\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    962\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    963\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m964\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    965\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    967\u001b[39m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/bioV/2023-Xu-Multimodal-Mouse-V1/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1367\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1360\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1361\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1362\u001b[39m             device,\n\u001b[32m   1363\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1364\u001b[39m             non_blocking,\n\u001b[32m   1365\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1366\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1367\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1368\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1369\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1370\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1371\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1372\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1373\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mAcceleratorError\u001b[39m: CUDA error: invalid device ordinal\nGPU device may be out of range, do you have enough GPUs?\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# 64 128 256\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "for file_id, num_neurons in [(\"070921_J553RT\", 68), (\"110421_J569LT\", 32), (\"101521_J559NC\", 49)]:\n",
    "    for shifter in [True]:\n",
    "        print(file_id)\n",
    "        args.vid_type = 'vid_mean'\n",
    "        args.file_id = file_id\n",
    "        args.num_neurons = num_neurons\n",
    "        args.shifter=shifter\n",
    "        args.best_train_path = \"/hdd/yuchen/trainAEshifter{}_{}_{}_{}.pth\".format(args.shifter, 1, args.vid_type, file_id)\n",
    "        args.best_val_path = \"/hdd/yuchen/valAEshifter{}_{}_{}_{}.pth\".format(args.shifter, 1, args.vid_type, file_id)\n",
    "    \n",
    "        model = AutoencoderPredictor777(num_neurons=args.num_neurons, \n",
    "                                        latent_size=args.vis_latent_dim,\n",
    "                                        c1=64, c2=128, c3=256).to(device)\n",
    "\n",
    "        train_loss_list, val_loss_list = train_model()\n",
    "\n",
    "    # np.save(\"/hdd/aiwenxu/net_weights_final/vis_autoencoder/loss_777_c1_64_{}.npy\".format(file_id), \n",
    "    #         np.array([train_loss_list, val_loss_list]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default is smoothing with 2 second, 48 ms per frame\n",
    "def smoothing_with_np_conv(nsp, size=int(2000/48)):\n",
    "    np_conv_res = []\n",
    "    for i in range(nsp.shape[1]):\n",
    "        np_conv_res.append(np.convolve(nsp[:, i], np.ones(size)/size, mode=\"same\"))        \n",
    "    np_conv_res = np.transpose(np.array(np_conv_res))\n",
    "    return np_conv_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, weights_path, dataset, device):\n",
    "\n",
    "    dl = DataLoader(dataset=dataset, batch_size=256, shuffle=False, num_workers=4)\n",
    "    \n",
    "    model.load_state_dict(torch.load(weights_path))\n",
    "\n",
    "    ground_truth_all = []\n",
    "    pred_all = []\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():      \n",
    "        \n",
    "        for (image, behav, spikes) in dl:\n",
    "            \n",
    "            image = image.to(device)\n",
    "            behav = behav.to(device)\n",
    "            \n",
    "            image = torch.squeeze(image, axis=1)\n",
    "            \n",
    "            pred, recon = model(image, behav)\n",
    "            \n",
    "            ground_truth_all.append(spikes.numpy())\n",
    "            pred_all.append(pred.cpu().numpy())\n",
    "    \n",
    "    return np.concatenate(pred_all, axis=0), np.concatenate(ground_truth_all, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "070921_J553RT\n",
      "30120 7540\n",
      "R2 0.777193\n",
      "MSE 0.071034\n",
      "mean corr, 0.555+-0.140\n",
      "max corr 0.816352\n",
      "min corr 0.177165\n",
      "070921_J553RT\n",
      "30120 7540\n",
      "R2 0.765927\n",
      "MSE 0.076842\n",
      "mean corr, 0.509+-0.144\n",
      "max corr 0.773564\n",
      "min corr 0.115169\n",
      "110421_J569LT\n",
      "32940 8240\n",
      "R2 0.224810\n",
      "MSE 0.112353\n",
      "mean corr, 0.370+-0.145\n",
      "max corr 0.651283\n",
      "min corr 0.048037\n",
      "110421_J569LT\n",
      "32940 8240\n",
      "R2 0.210233\n",
      "MSE 0.114998\n",
      "mean corr, 0.375+-0.148\n",
      "max corr 0.661694\n",
      "min corr 0.110536\n",
      "101521_J559NC\n",
      "42410 10610\n",
      "R2 0.727099\n",
      "MSE 0.097395\n",
      "mean corr, 0.521+-0.144\n",
      "max corr 0.822660\n",
      "min corr 0.242436\n",
      "101521_J559NC\n",
      "42410 10610\n",
      "R2 0.663554\n",
      "MSE 0.117084\n",
      "mean corr, 0.459+-0.140\n",
      "max corr 0.749090\n",
      "min corr 0.166779\n"
     ]
    }
   ],
   "source": [
    "# 64 128 256\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "for file_id, num_neurons in [(\"070921_J553RT\", 68), (\"110421_J569LT\", 32), (\"101521_J559NC\", 49)]:\n",
    "    \n",
    "    for shifter in [True, False]:\n",
    "        print(file_id)\n",
    "        args.vid_type = 'vid_mean'\n",
    "        args.file_id = file_id\n",
    "        args.num_neurons = num_neurons\n",
    "        args.shifter=shifter\n",
    "        args.best_train_path = \"/home/herbelinluke/Downloads/trainAEshifter{}_{}_{}_{}.pth\".format(args.shifter, 1, args.vid_type, file_id)\n",
    "        args.best_val_path = \"/home/herbelinluke/Downloads/valAEshifter{}_{}_{}_{}.pth\".format(args.shifter, 1, args.vid_type, file_id)\n",
    "        model = AutoencoderPredictor777(num_neurons=args.num_neurons, \n",
    "                                    latent_size=args.vis_latent_dim,\n",
    "                                    c1=64, c2=128, c3=256).to(device)\n",
    "    \n",
    "        train_ds, val_ds = load_train_val_ds()\n",
    "        test_ds = load_test_ds()\n",
    "    \n",
    "        pred, label = evaluate_model(model, weights_path=args.best_val_path, dataset=test_ds, device=device)\n",
    "        cor_array = cor_in_time(pred, label)\n",
    "    #     print(\"best val model on test dataset, {:.3f}+-{:.3f}\".format(np.mean(cor_array), np.std(cor_array)))\n",
    "        pred = smoothing_with_np_conv(pred)\n",
    "        label = smoothing_with_np_conv(label)\n",
    "        print(\"R2\", \"{:.6f}\".format(r2_score(label.T, pred.T)))\n",
    "        print(\"MSE\", \"{:.6f}\".format(mean_squared_error(label, pred)))\n",
    "        cor_array = cor_in_time(pred, label)\n",
    "        print(\"mean corr, {:.3f}+-{:.3f}\".format(np.mean(cor_array), np.std(cor_array)))\n",
    "        print(\"max corr\", \"{:.6f}\".format(np.max(cor_array)))\n",
    "        print(\"min corr\", \"{:.6f}\".format(np.min(cor_array)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
